{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hand Sign Detection via TensorFlow\n",
    "\n",
    "This project needs user to follow instructions precisely to run. \n",
    "Thus I recommend users to open the ReadMe file and\n",
    "follow its instructions + comments of this very jupyter notebook\n",
    "to run the project successfully.\n",
    "\n",
    "This file (\"hsd_tf_pipless\") only includes codes in order to run the project.\n",
    "For a comprehensive instruction along needed pip installs, please use \"hsd_tf_packages\".\n",
    "But if you already ran the project and every packages and needed libraries are available\n",
    "in your environment, you are good to go with this file.\n",
    "\n",
    "It is highly recommended to make a Python virtual environment on your machine,\n",
    "then run the projects. In the ReadMe file, I put a brief explanation about it.\n",
    "\n",
    "All the instructions and codes are developed based on Windows. Nontheless, it\n",
    "can be run on any platform with minor changes that I tried to mention in the comments.\n",
    "\n",
    "This file also assumes that environment includes any needed files and tools,\n",
    "which should be cloned in the \"hsd_tf_packages\" kernel.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import uuid\n",
    "\n",
    "# Signs Labelling\n",
    "\n",
    "# All the considered signs should be mentioned in below list\n",
    "sign_labels = ['like', 'dislike', 'victory', 'hi']\n",
    "# number of images per sign (will be used for the train and test)\n",
    "imgs_per_sign = 5\n",
    "\n",
    "# dir for directory\n",
    "imgs_dir = os.path.join('project_hsd', 'material', 'all_images', 'unlabeled_imgs')\n",
    "\n",
    "if not os.path.exists(imgs_dir):\n",
    "    !mkdir {imgs_dir}\n",
    "for sign in sign_labels:\n",
    "    dirs = os.path.join(imgs_dir, sign)\n",
    "    if not os.path.exists(dirs):\n",
    "        !mkdir {dirs}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already provide the photos, you don't need to run\n",
    "# this cell\n",
    "\n",
    "\n",
    "# Taking the pictures\n",
    "# This phase needs the user speed to pose \n",
    "# for pictures spontaneously.\n",
    "# A good lighting also runs a long way.\n",
    "\n",
    "# The quality of images is one of the most\n",
    "# important factors to develop an accurate model\n",
    "\n",
    "for sign in sign_labels:\n",
    "    pic = cv2.VideoCapture(0)\n",
    "    print('You have 10 seconds to pose for taking {} pictures '\n",
    "          'for {}'.format(imgs_per_sign, sign))\n",
    "    time.sleep(10)\n",
    "    for pic_num in range(imgs_per_sign):\n",
    "        print('Taking picture {}'.format(pic_num))\n",
    "        bool_val, img = pic.read()\n",
    "        pic_name = os.path.join(imgs_dir, sign, sign + '.' + '{}.jpg'.format(str(uuid.uuid1())))\n",
    "        cv2.imwrite(pic_name, img)\n",
    "        cv2.imshow('frame', img)\n",
    "        time.sleep(3)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "pic.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tool_dir = os.path.join('project_hsd', 'labeling_tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you labeled the photos, you don't need to run\n",
    "# this cell\n",
    "\n",
    "!cd {lab_tool_dir} && pyrcc5 -o libs/resources.py resources.qrc\n",
    "!cd {lab_tool_dir} && python labelImg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing folders for the TensorFlow and our system underlying model\n",
    "# Underlying model is a model than our object detection project uses as an offset\n",
    "# In fact, after completing our model, we will generate our own object detection system\n",
    "# which is specified for our handsign detection system.\n",
    "\n",
    "# In case that you restart the kernel which you probably will\n",
    "# import os again and run this cell\n",
    "import os\n",
    "\n",
    "## You can change the paths here, in order to export\n",
    "## new developed models. e.g. change the number of steps\n",
    "## and develop another model. You can even try 10000 steps\n",
    "## for a great accuracy.\n",
    "\n",
    "developed_model = 'handsign_od_model' \n",
    "underlying_model = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "underlying_model_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "glossary_generation_tf = 'convert_glossary_tf.py'\n",
    "sign_map = 'label_map.pbtxt'\n",
    "\n",
    "dirs = {\n",
    "    'material_dir': os.path.join('project_hsd', 'material'),\n",
    "    'conversion_dir': os.path.join('project_hsd','py_file'),\n",
    "    'tf_dir': os.path.join('project_hsd','tf_model'),\n",
    "    'glossary_dir': os.path.join('project_hsd', 'material','glossary'),\n",
    "    'img_dir': os.path.join('project_hsd', 'material','all_images'),\n",
    "    'my_model_dir': os.path.join('project_hsd', 'material','hsd_model'),\n",
    "    'underlying_model_dir': os.path.join('project_hsd', 'material','underlying_trained_model'),\n",
    "    'checkpoint_dir': os.path.join('project_hsd', 'material','hsd_model',developed_model), \n",
    "    'export_dir': os.path.join('project_hsd', 'material','hsd_model',developed_model, 'export'), \n",
    "    'js_export_dir':os.path.join('project_hsd', 'material','hsd_model',developed_model, 'js_export'), \n",
    "    'lite_export_dir':os.path.join('project_hsd', 'material','hsd_model',developed_model, 'lite_export'), \n",
    "    'PROTOC_PATH':os.path.join('project_hsd','protoc')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'model_pipline':os.path.join('project_hsd', 'material','hsd_model', developed_model, 'pipeline.config'),\n",
    "    'glossary_generation': os.path.join(dirs['conversion_dir'], glossary_generation_tf), \n",
    "    'signs_map': os.path.join(dirs['glossary_dir'], sign_map)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for path in dirs.values():\n",
    "    if not os.path.exists(path):\n",
    "        !mkdir {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_installation = os.path.join(dirs['tf_dir'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# This approves the complete installation of the TensorFlow \n",
    "# Please check the hsd_project ReadMe\n",
    "!python {approved_installation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import object_detection\n",
    "\n",
    "# Developing a label map for our signs \n",
    "\n",
    "sign_labs = [{'name':'like', 'id':1}, {'name':'dislike', 'id':2}, \n",
    "             {'name':'victory', 'id':3}, {'name':'hi', 'id':4}]\n",
    "\n",
    "with open(files['signs_map'], 'w') as f:\n",
    "    for label in sign_labs:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the cloned script (convert_glossary_tf.py)\n",
    "# This would make a training and testing record\n",
    "# These would train our model\n",
    "\n",
    "!python {files['glossary_generation']} -x {os.path.join(dirs['img_dir'], 'img_train')} -l {files['signs_map']} -o {os.path.join(dirs['glossary_dir'], 'img_train.record')} \n",
    "!python {files['glossary_generation']} -x {os.path.join(dirs['img_dir'], 'img_test')} -l {files['signs_map']} -o {os.path.join(dirs['glossary_dir'], 'img_test.record')} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In second runs, you don't need to run this cell\n",
    "## as it just copies the pretrained model to our training folder\n",
    "## On the other hand, you can use another pretrained model \n",
    "## compatible with our project to experiment\n",
    "\n",
    "# Copying the underlying model (already trained) to our training folder\n",
    "# This configure the way that our model will train data for \n",
    "# our specific task (here handsign detection)\n",
    "# This is in fact the architecture of our underlying model\n",
    "# It can be changed based on the model that we choose\n",
    "\n",
    "!copy {os.path.join(dirs['underlying_model_dir'], underlying_model, 'pipeline.config')} {os.path.join(dirs['checkpoint_dir'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "set_model = config_util.get_configs_from_pipeline_file(files['model_pipline'])\n",
    "set_model\n",
    "\n",
    "pipline_setting = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['model_pipline'], \"r\") as trained_model:                                                                                                                                                                                                                     \n",
    "    prototype = trained_model.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(prototype, pipline_setting)  \n",
    "    \n",
    "pipline_setting.model.ssd.num_classes = len(sign_labs)\n",
    "pipline_setting.train_config.batch_size = 4\n",
    "pipline_setting.train_config.fine_tune_checkpoint = os.path.join(dirs['underlying_model_dir'], underlying_model, 'checkpoint', 'ckpt-0')\n",
    "pipline_setting.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipline_setting.train_input_reader.label_map_path= files['signs_map']\n",
    "pipline_setting.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(dirs['glossary_dir'], 'img_train.record')]\n",
    "pipline_setting.eval_input_reader[0].label_map_path = files['signs_map']\n",
    "pipline_setting.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(dirs['glossary_dir'], 'img_test.record')]\n",
    "\n",
    "setting_str = text_format.MessageToString(pipline_setting)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(files['model_pipline'], \"wb\") as trained_model:                                                                                                                                                                                                                    \n",
    "    trained_model.write(setting_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training can start now\n",
    "\n",
    "# Since this takes some time, the best way is \n",
    "# not to run it on jupyter notebook since\n",
    "# it does not show any progress time\n",
    "# Thus we print the needed commpand ('expression' in this cell) and will open \n",
    "# a cmd with the same intrepreter (or virtual environment)\n",
    "# Paste it there, after 100 steps of training we will see an estimation\n",
    "# of each step\n",
    "\n",
    "# You can define the number of steps in the 'expression' variable\n",
    "\n",
    "\n",
    "\n",
    "model_training = os.path.join(dirs['tf_dir'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "\n",
    "# After copying the 'expression' in the cmd, you'll probably face \n",
    "# couple of errors, easy to solve though\n",
    "# first: Error: ValueError: numpy.ndarray size changed\n",
    "# Solution: uninstall and install pycocotools \n",
    "# pip uninstall pycocotools -y\n",
    "# pip install pycocotools\n",
    "# second: ModuleNotFoundError: no module named tensorflow-addons\n",
    "# Solution: pip install tensorflow-addons\n",
    "# Thirds: ModuleNotFoundError: no module named gin\n",
    "# Solution: install an old version of gin:\n",
    "# pip install gin-config==0.1.1\n",
    "\n",
    "# Probably good to go now, unless you're using a \n",
    "# gpu, in that sense, you should install the cudnn and cuda \n",
    "# versions that are compatible with the installed tensorflow\n",
    "# You have to install them before the training phase\n",
    "\n",
    "expression = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=3000\".format(model_training, dirs['checkpoint_dir'], files['model_pipline'])\n",
    "print(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying this expression in the cmd returns the performance matrix\n",
    "# ctrl + c will quit the results there in the cmd\n",
    "# train and eval folders consist loss matrix and evaluations matrix\n",
    "# From cmd, you can go to these folders and observe the evaluation\n",
    "# through tensorboard:\n",
    "# in cmd look for handsign_od_model where you can find mentioned folders\n",
    "# heading to train or eval you can do the same thing \n",
    "# Type this expression and enter: tensorboard --logdir=.\n",
    "# This expression returns this url: http://localhost:6006/\n",
    "# open it in your browser and observe the loss or evaluation matrices based\n",
    "# on the folder you're in (train and eval)\n",
    "\n",
    "expression = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(model_training, dirs['checkpoint_dir'],files['model_pipline'], dirs['checkpoint_dir'])\n",
    "print(expression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import object_detection\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# Checking our model performanceby experimenting on the test data\n",
    "# This runs an image detection\n",
    "\n",
    "# Assigning our model's pipeline config to the 'our_model'\n",
    "# in order to develop a detection model\n",
    "our_model = config_util.get_configs_from_pipeline_file(files['model_pipline'])\n",
    "hs_detection = model_builder.build(model_config=our_model['model'], is_training=False)\n",
    "\n",
    "\n",
    "hs_checkpoint = tf.compat.v2.train.Checkpoint(model=hs_detection)\n",
    "# From material-> hsd_model -> handsign_od_model -> \n",
    "# Check the greatest index for ckpt (checkpoint)\n",
    "# and put it in the below expression (last argument of the join method)\n",
    "hs_checkpoint.restore(os.path.join(dirs['checkpoint_dir'], 'ckpt-4')).expect_partial()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def obj_detection_func(image):\n",
    "    image, figures = hs_detection.preprocess(image)\n",
    "    model_pre_dict = hs_detection.predict(image, figures)\n",
    "    our_signs = hs_detection.postprocess(model_pre_dict, figures)\n",
    "    return our_signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image from our test image folder (img_test)\n",
    "# in order to apply our obj_detection_func on it\n",
    "classification_factor = label_map_util.create_category_index_from_labelmap(files['signs_map'])\n",
    "# Copy the image name instead of the below image name\n",
    "img_dir = os.path.join(dirs['img_dir'], 'img_test', 'hi.53892cc2-f2b9-11eb-9aa7-685d43928e06.jpg')\n",
    "\n",
    "# Mapping the image into a numpy array\n",
    "picture = cv2.imread(img_dir)\n",
    "np_pic = np.array(picture)\n",
    "\n",
    "tf_res = tf.convert_to_tensor(np.expand_dims(np_pic, 0), dtype=tf.float32)\n",
    "our_signs = obj_detection_func(tf_res)\n",
    "\n",
    "signs_num = int(our_signs.pop('num_detections'))\n",
    "our_signs = {key: value[0, :signs_num].numpy()\n",
    "             for key, value in our_signs.items()}\n",
    "our_signs['signs_num'] = signs_num\n",
    "\n",
    "# detection_classes should be ints\n",
    "our_signs['detection_classes'] = our_signs['detection_classes'].astype(np.int64)\n",
    "\n",
    "sign_set = 1\n",
    "np_pic_detected = np_pic.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            np_pic_detected,\n",
    "            our_signs['detection_boxes'],\n",
    "            our_signs['detection_classes'] + sign_set,\n",
    "            our_signs['detection_scores'],\n",
    "            classification_factor,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=.8,\n",
    "            agnostic_mode=False)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(np_pic_detected, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = cv2.VideoCapture(0)\n",
    "pic_width = int(pic.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "pic_height = int(pic.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while pic.isOpened():\n",
    "    ret, frame = pic.read()\n",
    "    image_np = np.array(frame)\n",
    "\n",
    "    tf_res = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    our_signs = obj_detection_func(tf_res)\n",
    "\n",
    "    signs_num = int(our_signs.pop('num_detections'))\n",
    "    our_signs = {key: value[0, :signs_num].numpy()\n",
    "                 for key, value in our_signs.items()}\n",
    "    our_signs['num_detections'] = signs_num\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    our_signs['detection_classes'] = our_signs['detection_classes'].astype(np.int64)\n",
    "\n",
    "    sign_set = 1\n",
    "    np_pic_detected = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        np_pic_detected,\n",
    "        our_signs['detection_boxes'],\n",
    "        our_signs['detection_classes'] + sign_set,\n",
    "        our_signs['detection_scores'],\n",
    "        classification_factor,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=.8,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection', cv2.resize(np_pic_detected, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        pic.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting different types of the trained model:\n",
    "# Below cell will export the result in the following folder:\n",
    "# handsign_detection_tensor_flow\\project_hsd\\material\\hsd_model\\handsign_od_model\\export\n",
    "# The pipeline.config file is the same as file that we have\n",
    "# have used in order to apply a pretrained model to develop on, namely\n",
    "# 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting different types of the trained model\n",
    "\n",
    "res_export = os.path.join(dirs['tf_dir'], 'research', 'object_detection', 'exporter_main_v2.py ')\n",
    "expression = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(res_export, files['model_pipline'], dirs['checkpoint_dir'], dirs['export_dir'])\n",
    "print(expression)\n",
    "!{expression}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model's json file \n",
    "\n",
    "expression = \"tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_features,detection_multiclass_scores,detection_scores,num_detections,raw_detection_boxes,raw_detection_scores' --output_format=tfjs_graph_model --signature_name=serving_default {} {}\".format(os.path.join(dirs['export_dir'], 'saved_model'), dirs['js_export_dir'])\n",
    "!{expression}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a lite version\n",
    "\n",
    "res_lite = os.path.join(dirs['tf_dir'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')\n",
    "expression = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(res_lite, files['model_pipline'], dirs['checkpoint_dir'], dirs['lite_export_dir'])\n",
    "!{expression}\n",
    "\n",
    "res_lite_dir = os.path.join(dirs['lite_export_dir'], 'saved_model')\n",
    "model_lite = os.path.join(dirs['lite_export_dir'], 'saved_model', 'detect.tflite')\n",
    "\n",
    "expression = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(res_lite_dir, model_lite, )\n",
    "!{expression}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
